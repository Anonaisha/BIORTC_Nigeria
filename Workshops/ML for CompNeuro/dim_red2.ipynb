{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "When working with neuroscientific data, an essential first step before modeling or analysis is **preprocessing**. This includes preparing the data in a way that allows us to extract meaningful patterns while reducing noise and complexity.\n",
    "\n",
    "One major aspect of preprocessing is **dimensionality reduction**. Neural data is often high-dimensional and complex. Dimensionality reduction techniques help simplify this data by projecting it into a lower-dimensional space, making it easier to visualize, analyze, and interpret.\n",
    "\n",
    "There are many techniques available, each with different assumptions and use cases. Choosing the right one depends on your **research question**, your **objectives**, and the **nature of your data**.\n",
    "\n",
    "In this tutorial, we will introduce a few widely used dimensionality reduction methods. These include both linear and non-linear approaches:\n",
    "\n",
    "### Techniques we will cover\n",
    "\n",
    "**Linear method:**\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "**Non-linear methods:**\n",
    "- t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "- UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "**Non-linear, learning-based method:**\n",
    "- Variational Autoencoders (VAEs)\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- The basics of Principal Component Analysis (PCA)\n",
    "- How to apply PCA for dimensionality reduction\n",
    "- How to reduce dimensionality using t-SNE and UMAP\n",
    "- How to implement a simple Variational Autoencoder (VAE)\n",
    "- How to apply these techniques to real neural data recorded from the prefrontal cortex of a monkey during a working memory task\n",
    "- How to decode the memorized cue from the monkey's brain activity using reduced data\n",
    "\n",
    "A short presentation will give you an overview of the topic. If you're interested in diving deeper, explore the following resources:\n",
    "\n",
    "### Further resources\n",
    "\n",
    "**Detailed tutorial**  \n",
    "Neuromatch Academy:  \n",
    "[https://compneuro.neuromatch.io/tutorials/W1D4_DimensionalityReduction/student/W1D4_Intro.html](https://compneuro.neuromatch.io/tutorials/W1D4_DimensionalityReduction/student/W1D4_Intro.html)\n",
    "\n",
    "**Mathematical foundations of PCA**  \n",
    "Covariance matrices and eigenvalues (YouTube):  \n",
    "[https://youtube.com/watch?v=-f6T9--oM0E](https://youtube.com/watch?v=-f6T9--oM0E)\n",
    "\n",
    "**Intuition for PCA**  \n",
    "[https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "\n",
    "**Intuition for t-SNE**  \n",
    "[https://towardsdatascience.com/t-sne-machine-learning-algorithm-a-great-tool-for-dimensionality-reduction-in-python-ec01552f1a1e](https://towardsdatascience.com/t-sne-machine-learning-algorithm-a-great-tool-for-dimensionality-reduction-in-python-ec01552f1a1e)\n",
    "\n",
    "**UMAP overview**  \n",
    "[https://towardsdatascience.com/umap-dimensionality-reduction-an-incredibly-robust-machine-learning-algorithm-b5acb01de568](https://towardsdatascience.com/umap-dimensionality-reduction-an-incredibly-robust-machine-learning-algorithm-b5acb01de568)\n",
    "\n",
    "**Understanding VAEs**  \n",
    "[https://towardsdatascience.com/variational-bayes-4abdd9eb5c12](https://towardsdatascience.com/variational-bayes-4abdd9eb5c12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "We begin with one of the most commonly used dimensionality reduction techniques: Principal Component Analysis (PCA). PCA is a linear method that finds the directions (principal components) along which the variance of the data is maximized.\n",
    "\n",
    "PCA is useful because it simplifies the data by projecting it onto a smaller number of dimensions while preserving as much variance as possible. This often makes it easier to visualize, analyze, and interpret complex neural data.\n",
    "\n",
    "### Loading and Inspecting the Data\n",
    "\n",
    "We will use neural data recorded from the prefrontal cortex of a monkey performing a working memory task.\n",
    "\n",
    "The task structure is as follows: a cue is presented on the screen for 0.5 seconds, followed by a 3-second delay period—a relatively long delay compared to many primate neurophysiology studies. The task involved eight stimuli representing angular locations (0º, 45º, 90º, 135º, 180º, 225º, 270º, and 315º).\n",
    "\n",
    "Only trials in which the monkeys performed correctly were included in the analysis.\n",
    "\n",
    "The data is organized in a dictionary, with neural activity arranged in a matrix where each row corresponds to a trial and each column corresponds to a neuron. Each value represents the neuron's average activity during a defined time window within the trial.\n",
    "\n",
    "Make sure, the data we are working with is in the correct dictionary: ...\n",
    "\n",
    "For detailed information on the dataset and experimental design, refer to the original paper:  \n",
    "[Murray et al., 2017, PNAS](https://raw.githubusercontent.com/wimmerlab/MBC_data_analysis/main/A7_DimensionalityReduction/Murray_PNAS_2017.pdf)\n",
    "\n",
    "the main goal of this analysis is to test whether the population of prefrontal cortex neurons maintains a stable, low-dimensional, representation of the cue during the delay period.\n",
    "We will reproduce some of the key figures from the Murray et al. 2017 paper, with some simplifications. - ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#load the preprocessed dataset\n",
    "data = np.load('monkey_prefrontal_data.npy')  #shape: (n_trials, n_neurons)\n",
    "labels = np.load('monkey_cue_labels.npy')     #categorical labels: cue location, etc.\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA\n",
    "We will reduce the data dimensionality by projecting it onto the first two principal components. This will allow us to visualize the trials in a two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA\n",
    "We will reduce the data dimensionality by projecting it onto the first two principal components. This will allow us to visualize the trials in a two-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#apply PCA -  this is the only code you need to do all the steps of PCA in once! Implementiation is really easy, but make sure you carefully think about why you apply, in terms of your data and research objectives, and what nr of components to choose!\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='tab10', alpha=0.8)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.title('Neural data projected onto first two principal components')\n",
    "plt.colorbar(scatter, label='Cue condition')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this plot show? Try playing around witrh the number of components. What does it do, why?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
