{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f6d70e",
   "metadata": {},
   "source": [
    "# Classifiers workshop\n",
    "\n",
    "In this workshop we will continue working with calcium imaging data. \n",
    "\n",
    "In the calcium workshop you learned about the Peri-Stimulus Time Histogram. Again, PSTH is a way to visualize the average neural response aligned to a specific event like the stimulus onset, reward, or a behavioral cue.\n",
    "\n",
    " But when you have hundreds of neurons over several trials, visualising one by one and deciding what is the most likely function is extremely time consuming and inefficient. Luckily there are several approaches that we can use to classify the neurons, using machine learning techniques we learned during the lectures. \n",
    "\n",
    " In this workshop, we will continue doing some exploratory data analysis by using 2 techniques we learned during the lectures: different types of dimensionality reduction and K-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install umap-learn\n",
    "%pip install scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly\n",
    "%pip install nbformat --upgrade\n",
    "%pip install jupyterlab \"ipywidgets>=7.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and load the right libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.integrate import simpson\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02121826",
   "metadata": {},
   "source": [
    "First, we are going to consolidate what we learned in the calcium data analysis workshop. Let's build a pipeline to sort the neuron responses into its 4 different categories. \n",
    "\n",
    "We are going to try 2 approaches to see what gives us better clusters: \n",
    "\n",
    "- a raw data dataset, containing the psths for each neuron (so for each neuron we would have an array like [hit_psth, miss_psth, cr_psth, fa_psth]); \n",
    "  \n",
    "- a dataset containing a summary of the features, such as the peak for each psth, the area under the curve, and how long after the stimulus the peak occurs (so for each neuron we would have an array like ['hit_peak', 'miss_peak', 'fa_peak','cr_peak', \"hit_latency\",\"miss_latency\",\"fa_latency\",\"cr_latency\",\"hit_auc\",\"miss_auc\",\"fa_auc\", \"cr_auc\"]). \n",
    "\n",
    "\n",
    "Since we \"don't know what to expect\", let's compare and see what the best methods are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to create a dataset with the information we need\n",
    "\n",
    "data_directory = \"S1_trained\" # Path to our dataset\n",
    "output_csv_stats = \"neuron_response_features.csv\"\n",
    "output_csv_raw_psths = \"neuron_responses_raw.csv\"\n",
    "\n",
    "\n",
    "neuron_features = []\n",
    "\n",
    "neuron_features_raw= []\n",
    "\n",
    "\n",
    "\n",
    "for neuron_file in os.listdir(data_directory):\n",
    "    if not neuron_file.endswith('.mat'):\n",
    "        continue\n",
    "\n",
    "    mat = loadmat(os.path.join(data_directory, neuron_file))\n",
    "\n",
    "    \n",
    "    # load the variables\n",
    "    dff = mat['dff'].flatten()\n",
    "    cues = mat['cues'].flatten()\n",
    "    reward = mat['reward'].flatten()\n",
    "    punish =mat['punish'].flatten()\n",
    "    go= mat['go'].flatten()\n",
    "    nogo= mat['nogo'].flatten()\n",
    "\n",
    "    stimtimes = list(np.where(cues ==1)[0])\n",
    "\n",
    "    # get the number of trials \n",
    "    numtrials = len(stimtimes)\n",
    "\n",
    "    # get the duration of the trial.  The trial duration is the time between one stimulus and the other. \n",
    "    #think about it, you have a list of time points where the cues happen. How can you get the amount of time between one cue and the next? \n",
    "    trial_duration = stimtimes[1] - stimtimes[0]  # assumes constant duration\n",
    "\n",
    "    baseline = 2\n",
    "    post_stim_period = trial_duration - baseline - 1\n",
    "\n",
    "\n",
    "    # Allocate Containers\n",
    "    hit_waves = []\n",
    "    miss_waves = [] \n",
    "    fa_waves = [] \n",
    "    cr_waves = [] \n",
    "\n",
    "\n",
    "    # Sort Trials by Type\n",
    "\n",
    "    #set the counter to all these to 0\n",
    "    hit = 0\n",
    "    miss = 0\n",
    "    fa = 0\n",
    "    cr = 0\n",
    "\n",
    "    for itrial in range(numtrials - 1):\n",
    "        \n",
    "        istimstart = stimtimes[itrial] # time of the current trial\n",
    "        next_stim = stimtimes[itrial + 1] # time of the next trial\n",
    "        start_idx = istimstart - baseline\n",
    "        end_idx = istimstart + post_stim_period\n",
    "\n",
    "        #segment of dff\n",
    "        segment = dff[start_idx:end_idx]\n",
    "\n",
    "        # checks if the stimulus is Go\n",
    "        if go[istimstart] == 1:\n",
    "            # if the stimulus is Go and there is a reward, it means that the mouse licked for the right stimulus! Hurrah!\n",
    "            if np.any(reward[istimstart:next_stim] ==1 ):\n",
    "                hit_waves.append(segment)\n",
    "                hit+=1\n",
    "            else:\n",
    "            #if the stimulus is Go and the mouse did not get a reward, it's a miss\n",
    "                miss_waves.append(segment)\n",
    "                miss+=1\n",
    "\n",
    "        # check if the stimulus is NoGo\n",
    "        elif nogo[istimstart] ==1:\n",
    "            # if the mouse was punished, it means it licked, therefore false alarm\n",
    "            if np.any(punish[istimstart:next_stim] ==1 ):\n",
    "                fa_waves.append(segment)\n",
    "                fa+=1\n",
    "            else:\n",
    "            # otherwise it correctly rejected the NoGo stimulus\n",
    "                cr_waves.append(segment)\n",
    "                cr+=1\n",
    "\n",
    "\n",
    "    #  Lick-Aligned Waveforms\n",
    "\n",
    "    # both hit and fa involve the mouse licking (miss and correct rejections means that the mouse doesn't lick)\n",
    "    hit_lick_waves = []\n",
    "    fa_lick_waves = []\n",
    "\n",
    "    #same thing as before \n",
    "    for itrial in range(1, numtrials - 1):\n",
    "        istimstart = stimtimes[itrial]\n",
    "        next_stim = stimtimes[itrial + 1]\n",
    "        \n",
    "    # here it's a bit different, but we basically get whether the mouse was rewarded (aka licked correctly)\n",
    "        if go[istimstart] == 1:\n",
    "            licktimes = np.where(reward[istimstart:next_stim] > 0)[0] # the reward is given if the mouse licks correctly \n",
    "            if len(licktimes) > 0:\n",
    "                lick_index = istimstart + licktimes[0]\n",
    "                start = lick_index - trial_duration // 2\n",
    "                end = lick_index + trial_duration // 2\n",
    "                segment = dff[start:end]\n",
    "                #print(lick_index, licktimes, licktimes[0])\n",
    "\n",
    "                hit_lick_waves.append(segment)\n",
    "        #or we get whether the mouse was punished (licked but shouldn't have)\n",
    "        elif nogo[istimstart] > 0:\n",
    "            licktimes = np.where(punish[istimstart:next_stim] > 0)[0]\n",
    "            if len(licktimes) > 0:\n",
    "                lick_index = istimstart + licktimes[0]\n",
    "                start = lick_index - trial_duration // 2\n",
    "                end = lick_index + trial_duration // 2\n",
    "                segment = dff[start:end]\n",
    "                fa_lick_waves.append(segment)\n",
    "\n",
    "\n",
    "\n",
    "    hit_lick_waves = np.array(hit_lick_waves).T if hit_lick_waves else np.empty((trial_duration, 0))\n",
    "    fa_lick_waves = np.array(fa_lick_waves).T if fa_lick_waves else np.empty((trial_duration, 0))\n",
    "\n",
    "\n",
    "\n",
    "    # we are now writing a function that takes the waves and computes important information such as psth, the peak of the wave, the area under the curve (auc), and the latency at which the peak occurs. \n",
    "\n",
    "    \n",
    "    def compute_features(waves):\n",
    "        if len(waves) == 0:\n",
    "            return np.zeros(post_stim_period + baseline), 0, 0, 0\n",
    "        psth = np.mean(waves, axis=0)\n",
    "        peak = np.max(psth)\n",
    "        auc = simpson(psth)\n",
    "        latency = np.argmax(psth)\n",
    "        return psth, peak, auc, latency \n",
    "\n",
    "    hit_psth, hit_peak, hit_auc, hit_latency = compute_features(hit_waves)\n",
    "    miss_psth, miss_peak, miss_auc, miss_latency  = compute_features(miss_waves)\n",
    "    fa_psth, fa_peak, fa_auc, fa_latency  = compute_features(fa_waves)\n",
    "    cr_psth, cr_peak, cr_auc, cr_latency  = compute_features(cr_waves)\n",
    "\n",
    "\n",
    "    # dataset 1: RAW PSTHs\n",
    "    # Concatenate all PSTHs a\n",
    "    raw_vector = np.concatenate([hit_psth, miss_psth, fa_psth, cr_psth])\n",
    "    neuron_features_raw.append({'neuron_id': neuron_file, 'features': raw_vector})\n",
    "\n",
    "\n",
    "   # dataset 2: features extracted from the data\n",
    "   #(peak, area under the curve, latency of the peak )\n",
    "    \n",
    "    neuron_features.append({\n",
    "        'neuron_id': neuron_file,\n",
    "        'hit_peak': hit_peak,\n",
    "        'hit_auc': hit_auc,\n",
    "        'hit_latency': hit_latency,\n",
    "        'miss_peak': miss_peak,\n",
    "        'miss_auc': miss_auc,\n",
    "        \"miss_latency\": miss_latency,\n",
    "        'fa_peak': fa_peak,\n",
    "        'fa_auc': fa_auc,\n",
    "        \"fa_latency\": fa_latency,\n",
    "        'cr_peak': cr_peak,\n",
    "        'cr_auc': cr_auc,\n",
    "        \"cr_latency\":cr_latency\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "#store the two different datasets \n",
    "\n",
    "raw_df = pd.DataFrame([{\n",
    "    'neuron_id': entry['neuron_id'],\n",
    "    **{f'feat_{i}': val for i, val in enumerate(entry['features'])}\n",
    "} for entry in neuron_features_raw])\n",
    "\n",
    "#print(raw_df)\n",
    "raw_df.to_csv(output_csv_raw_psths , index=False)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(neuron_features)\n",
    "#print(df)\n",
    "df.to_csv(output_csv_stats, index=False)\n",
    "print(f\"Feature extraction complete. Saved to {output_csv_stats}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check a summary of the first --descriptive-- dataframe\n",
    "summary_df = pd.read_csv(\"neuron_response_features.csv\")\n",
    "summary_df[['hit_peak', 'miss_peak', 'fa_peak','cr_peak', \"hit_latency\",\"miss_latency\",\"fa_latency\",\"cr_latency\",\"hit_auc\",\"miss_auc\",\"fa_auc\", \"cr_auc\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check a summary of what the raw data dataset looks like\n",
    "raw_df = pd.read_csv(\"neuron_responses_raw.csv\")\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c101e8d",
   "metadata": {},
   "source": [
    "Now that we have our datasets `neuron_response_features.csv` and `neuron_responses_raw.csv`, let's check the optimal amount of clusters using the elbow method! \n",
    "\n",
    "As you remember from the lecture, the elbow method indicates the best number of cluster, which is at the point where there is an \"elbow\" in the inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "df = pd.read_csv(\"neuron_response_features.csv\")\n",
    "\n",
    "# Drop non-numeric columns ( we don't need the neuron_id)\n",
    "features = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "# Elbow -- we are going to see the inertia score for each k in a range from 2 to 10\n",
    "inertia = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe51902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"neuron_responses_raw.csv\")\n",
    "\n",
    "# Drop non-numeric columns ( we don't need the neuron_id)\n",
    "features = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "# Elbow \n",
    "inertia = []\n",
    "\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a9f7a",
   "metadata": {},
   "source": [
    "Now that we found the optimal amount of clusters, let's apply dimensionality reduction on our datasets to see if we get some clusters!\n",
    "\n",
    "Which type of dimensionality reduction? Why not trying them all and compare the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare and standardize datasets\n",
    "feature_data = summary_df.drop(columns=['neuron_id'])\n",
    "raw_data = raw_df.drop(columns=['neuron_id'])\n",
    "\n",
    "scaler_feat = StandardScaler()\n",
    "X_feat = scaler_feat.fit_transform(feature_data)\n",
    "\n",
    "scaler_raw = StandardScaler()\n",
    "X_raw = scaler_raw.fit_transform(raw_data)\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "pca_feat = PCA(n_components=2).fit_transform(X_feat)\n",
    "tsne_feat = TSNE(n_components=2, random_state=42).fit_transform(X_feat)\n",
    "umap_feat = umap.UMAP(random_state=42).fit_transform(X_feat)\n",
    "\n",
    "pca_raw = PCA(n_components=2).fit_transform(X_raw)\n",
    "tsne_raw = TSNE(n_components=2, random_state=42).fit_transform(X_raw)\n",
    "umap_raw = umap.UMAP(random_state=42).fit_transform(X_raw)\n",
    "\n",
    "# Apply KMeans clustering (default to 4 clusters)\n",
    "kmeans_feat = KMeans(n_clusters=4, random_state=42).fit(X_feat)\n",
    "kmeans_raw = KMeans(n_clusters=4, random_state=42).fit(X_raw)\n",
    "\n",
    "# Add cluster labels to datasets\n",
    "summary_df['cluster'] = kmeans_feat.labels_\n",
    "raw_df['cluster'] = kmeans_raw.labels_\n",
    "\n",
    "# Plotting function\n",
    "def plot_embeddings(embeddings, labels, title, ax):\n",
    "    sns.scatterplot(x=embeddings[:,0], y=embeddings[:,1], hue=labels, palette='Set2', s=40, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "plot_embeddings(pca_feat, kmeans_feat.labels_, \"Feature PCA (4 Clusters)\", axes[0, 0])\n",
    "plot_embeddings(pca_raw, kmeans_raw.labels_, \"Raw PCA (4 Clusters)\", axes[0, 1])\n",
    "plot_embeddings(tsne_feat, kmeans_feat.labels_, \"Feature t-SNE (4 Clusters)\", axes[1, 0])\n",
    "plot_embeddings(tsne_raw, kmeans_raw.labels_, \"Raw t-SNE (4 Clusters)\", axes[1, 1])\n",
    "plot_embeddings(umap_feat, kmeans_feat.labels_, \"Feature UMAP (4 Clusters)\", axes[2, 0])\n",
    "plot_embeddings(umap_raw, kmeans_raw.labels_, \"Raw UMAP (4 Clusters)\", axes[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efab0e6",
   "metadata": {},
   "source": [
    "So far PCA seems the clearest, while we might need to increase the dimensionality to get a better understanding of the t-sne and umap clusters. \n",
    "\n",
    "But first, in PCA each principal component (PC) is a linear combination of the original features.This allows us to see how much the features contribute to each Component. \n",
    "\n",
    "t-SNE and UMAP are nonlinear, and we therefore cannot interpret their dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694405de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop non-numerical columns explicitly\n",
    "feature_names = summary_df.drop(columns=[\"neuron_id\", \"preferred_trial\"], errors=\"ignore\").select_dtypes(include=np.number).columns\n",
    "\n",
    "# Prepare the data used for PCA\n",
    "X_feat = summary_df[feature_names].values\n",
    "\n",
    "# === 1. Fit PCA model and transform data ===\n",
    "pca_model_feat = PCA(n_components=3)\n",
    "pca_feat = pca_model_feat.fit_transform(X_feat)\n",
    "\n",
    "# === 2. Explained variance per PC ===\n",
    "print(\"Explained variance ratio (percent of total variance):\")\n",
    "for i, ratio in enumerate(pca_model_feat.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.2%}\")\n",
    "\n",
    "# === 3. Top features in PC1 ===\n",
    "pc1_weights = pd.Series(pca_model_feat.components_[0], index=feature_names).abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 features contributing to PC1 (most variance):\")\n",
    "print(pc1_weights.head(5))\n",
    "\n",
    "# === 4. Average contribution across PC1–PC3 ===\n",
    "avg_contributions = pd.DataFrame(\n",
    "    np.abs(pca_model_feat.components_[:3]), \n",
    "    columns=feature_names\n",
    ").mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 average contributors across PC1–PC3:\")\n",
    "print(avg_contributions.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323db49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now do the same for the raw_data pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e936fa3",
   "metadata": {},
   "source": [
    "Let's give umap and t-sne a fair chance, and increase the dimensions and see if the clusters are clearer now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec71b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and scale data\n",
    "features_df = pd.read_csv(\"neuron_response_features.csv\")\n",
    "raw_df = pd.read_csv(\"neuron_responses_raw.csv\")\n",
    "\n",
    "X_feat = StandardScaler().fit_transform(features_df.drop(columns=['neuron_id']))\n",
    "X_raw = StandardScaler().fit_transform(raw_df.drop(columns=['neuron_id']))\n",
    "\n",
    "# Optional: PCA reduction before t-SNE/UMAP\n",
    "X_raw_pca50 = PCA(n_components=50).fit_transform(X_raw)\n",
    "\n",
    "# 3D t-SNE and UMAP\n",
    "tsne_feat_3d = TSNE(n_components=3, random_state=42).fit_transform(X_feat)\n",
    "tsne_raw_3d = TSNE(n_components=3, random_state=42).fit_transform(X_raw_pca50)\n",
    "umap_feat_3d = umap.UMAP(n_components=3, random_state=42).fit_transform(X_feat)\n",
    "umap_raw_3d = umap.UMAP(n_components=3, random_state=42).fit_transform(X_raw)\n",
    "\n",
    "# Clustering\n",
    "kmeans_feat = KMeans(n_clusters=4, random_state=42).fit(X_feat)\n",
    "kmeans_raw = KMeans(n_clusters=4, random_state=42).fit(X_raw)\n",
    "\n",
    "# Plotting function\n",
    "def plot_3d(data, labels, title):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(data[:,0], data[:,1], data[:,2], c=labels, cmap='Set2', s=40)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Dim 1\")\n",
    "    ax.set_ylabel(\"Dim 2\")\n",
    "    ax.set_zlabel(\"Dim 3\")\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "plot_3d(tsne_feat_3d, kmeans_feat.labels_, \"3D t-SNE - Features\")\n",
    "plot_3d(tsne_raw_3d, kmeans_raw.labels_, \"3D t-SNE - Raw\")\n",
    "plot_3d(umap_feat_3d, kmeans_feat.labels_, \"3D UMAP - Features\")\n",
    "plot_3d(umap_raw_3d, kmeans_raw.labels_, \"3D UMAP - Raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987fc4a",
   "metadata": {},
   "source": [
    "Fun fact! You can also plot interactive plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81783eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# Load data\n",
    "features_df = pd.read_csv(\"neuron_response_features.csv\")\n",
    "raw_df = pd.read_csv(\"neuron_responses_raw.csv\")\n",
    "\n",
    "# Prepare\n",
    "X_feat = StandardScaler().fit_transform(features_df.drop(columns=[\"neuron_id\"]))\n",
    "X_raw = StandardScaler().fit_transform(raw_df.drop(columns=[\"neuron_id\"]))\n",
    "X_raw_pca50 = PCA(n_components=50).fit_transform(X_raw)\n",
    "\n",
    "# Clustering\n",
    "kmeans_feat = KMeans(n_clusters=4, random_state=42).fit(X_feat)\n",
    "kmeans_raw = KMeans(n_clusters=4, random_state=42).fit(X_raw)\n",
    "\n",
    "# Dimensionality reduction\n",
    "tsne_feat = TSNE(n_components=3, random_state=42).fit_transform(X_feat)\n",
    "tsne_raw = TSNE(n_components=3, random_state=42).fit_transform(X_raw_pca50)\n",
    "umap_feat = umap.UMAP(n_components=3, random_state=42).fit_transform(X_feat)\n",
    "umap_raw = umap.UMAP(n_components=3, random_state=42).fit_transform(X_raw)\n",
    "\n",
    "# Create interactive 3D plot\n",
    "def plot_3d_plotly(embedding, labels, title):\n",
    "    df = pd.DataFrame(embedding, columns=[\"x\", \"y\", \"z\"])\n",
    "    df[\"cluster\"] = labels\n",
    "    fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", color=df[\"cluster\"].astype(str),\n",
    "                        title=title, width = 1000, height = 800, labels={\"color\": \"Cluster\"})\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.show()\n",
    "\n",
    "# Run visualizations\n",
    "plot_3d_plotly(tsne_feat, kmeans_feat.labels_, \"3D t-SNE - Features\")\n",
    "plot_3d_plotly(tsne_raw, kmeans_raw.labels_, \"3D t-SNE - Raw\")\n",
    "plot_3d_plotly(umap_feat, kmeans_feat.labels_, \"3D UMAP - Features\")\n",
    "plot_3d_plotly(umap_raw, kmeans_raw.labels_, \"3D UMAP - Raw\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136b0c1",
   "metadata": {},
   "source": [
    "Now that you have completed this, you can see that there are different clusters emerging from different methods. \n",
    "The next step is up to you, but once you identify clusters, it's easier to get an idea of what classifier would be best (eg, would a binary classifier be a good fit? or if there are multiple categories, perhaps a random forest? )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malawi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
